{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9d6626",
   "metadata": {},
   "source": [
    "# Steps 3 & 4: Querying a Completion Model with a Custom Text Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0478d18",
   "metadata": {},
   "source": [
    "Add your API key to the cell below then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "186b2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "f = open(\"../../openai_app.key\", \"rt\")\n",
    "openai.api_key = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa50906",
   "metadata": {},
   "source": [
    "The code below loads in the data sorted by cosine distance that you previously created. Run it as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fd3a3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>distances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The USGS Prompt Assessment of Global Earthquak...</td>\n",
       "      <td>[-0.00565063 -0.02241383  0.00731853 ...  0.00...</td>\n",
       "      <td>0.087576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There was widespread damage in an area of abou...</td>\n",
       "      <td>[-0.00294703 -0.02162242 -0.01029019 ...  0.00...</td>\n",
       "      <td>0.088664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On 6 February 2023, at 04:17 TRT (01:17 UTC), ...</td>\n",
       "      <td>[-0.00861731 -0.01604001 -0.01179847 ... -0.00...</td>\n",
       "      <td>0.117818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>The Turkish Government was criticized on socia...</td>\n",
       "      <td>[-0.00018135  0.00045083 -0.00526761 ... -0.00...</td>\n",
       "      <td>0.122369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Mahase, Elisabeth (7 February 2023). \"Death to...</td>\n",
       "      <td>[-0.01100119 -0.02313346 -0.00543663 ...  0.00...</td>\n",
       "      <td>0.124095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NATO secretary-general Jens Stoltenberg said t...</td>\n",
       "      <td>[-0.00911843 -0.03215307  0.00483754 ... -0.00...</td>\n",
       "      <td>0.216761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Arab League secretary-general Ahmed Aboul Ghei...</td>\n",
       "      <td>[-0.0294337  -0.00111019  0.02146475 ... -0.03...</td>\n",
       "      <td>0.221374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>President Erdoğan declared seven days of natio...</td>\n",
       "      <td>[-0.00719139 -0.01002615  0.00472568 ...  0.01...</td>\n",
       "      <td>0.226036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ReliefWeb's  main page  for this event.</td>\n",
       "      <td>[-2.54744310e-02 -7.11684162e-03  2.90931948e-...</td>\n",
       "      <td>0.227360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>On 9 February, the Council of Higher Education...</td>\n",
       "      <td>[-0.00310741 -0.02646307  0.00548307 ... -0.01...</td>\n",
       "      <td>0.233063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "20  The USGS Prompt Assessment of Global Earthquak...   \n",
       "2   There was widespread damage in an area of abou...   \n",
       "0   On 6 February 2023, at 04:17 TRT (01:17 UTC), ...   \n",
       "37  The Turkish Government was criticized on socia...   \n",
       "51  Mahase, Elisabeth (7 February 2023). \"Death to...   \n",
       "..                                                ...   \n",
       "34  NATO secretary-general Jens Stoltenberg said t...   \n",
       "31  Arab League secretary-general Ahmed Aboul Ghei...   \n",
       "41  President Erdoğan declared seven days of natio...   \n",
       "56            ReliefWeb's  main page  for this event.   \n",
       "28  On 9 February, the Council of Higher Education...   \n",
       "\n",
       "                                           embeddings  distances  \n",
       "20  [-0.00565063 -0.02241383  0.00731853 ...  0.00...   0.087576  \n",
       "2   [-0.00294703 -0.02162242 -0.01029019 ...  0.00...   0.088664  \n",
       "0   [-0.00861731 -0.01604001 -0.01179847 ... -0.00...   0.117818  \n",
       "37  [-0.00018135  0.00045083 -0.00526761 ... -0.00...   0.122369  \n",
       "51  [-0.01100119 -0.02313346 -0.00543663 ...  0.00...   0.124095  \n",
       "..                                                ...        ...  \n",
       "34  [-0.00911843 -0.03215307  0.00483754 ... -0.00...   0.216761  \n",
       "31  [-0.0294337  -0.00111019  0.02146475 ... -0.03...   0.221374  \n",
       "41  [-0.00719139 -0.01002615  0.00472568 ...  0.01...   0.226036  \n",
       "56  [-2.54744310e-02 -7.11684162e-03  2.90931948e-...   0.227360  \n",
       "28  [-0.00310741 -0.02646307  0.00548307 ... -0.01...   0.233063  \n",
       "\n",
       "[61 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"distances.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757505cb",
   "metadata": {},
   "source": [
    "## TODO 1: Build the Custom Text Prompt\n",
    "\n",
    "Run the cell below as-is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2c16528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "# Create a tokenizer that is designed to align with our embeddings\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "token_limit = 1000\n",
    "USER_QUESTION = \"\"\"What were the estimated damages of the 2023 \\\n",
    "Turkey-Syria earthquake?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c0ca1",
   "metadata": {},
   "source": [
    "Now your task is to compose the custom text prompt.\n",
    "\n",
    "The overall structure of the prompt should look like this:\n",
    "\n",
    "```\n",
    "Answer the question based on the context below, and if the\n",
    "question can't be answered based on the context, say \"I don't\n",
    "know\"\n",
    "\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "```\n",
    "\n",
    "In the place marked `context`, provide as much information from `df['text']` as possible without exceeding `token_limit`. In the place marked `question`, add `USER_QUESTION`.\n",
    "\n",
    "Your overall goal is to create a string called `prompt` that contains all of the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de013ac",
   "metadata": {},
   "source": [
    "If you're getting stuck, you can click to reveal the solution then copy and paste this into the cell below.\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer\"><strong>Solution (click to show/hide)</strong></summary>\n",
    "\n",
    "```python\n",
    "# Count the number of tokens in the prompt template and question\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the \n",
    "question can't be answered based on the context, say \n",
    "\"I don't know\"\n",
    "\n",
    "Context: \n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "token_count = len(tokenizer.encode(prompt_template)) + \\\n",
    "                        len(tokenizer.encode(USER_QUESTION))\n",
    "\n",
    "# Create a list to store text for context\n",
    "context_list = []\n",
    "\n",
    "# Loop over rows of the sorted dataframe\n",
    "for text in df[\"text\"].values:\n",
    "    \n",
    "    # Append text to context_list if there is enough room\n",
    "    token_count += len(tokenizer.encode(text))\n",
    "    if token_count <= token_limit:\n",
    "        context_list.append(text)\n",
    "    else:\n",
    "        # Break once we're over the token limit\n",
    "        break\n",
    "\n",
    "# Use string formatting to complete the prompt\n",
    "prompt = prompt_template.format(\n",
    "    \"\\n\\n###\\n\\n\".join(context_list),\n",
    "    USER_QUESTION\n",
    ")\n",
    "print(prompt)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2b405d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "f = open(\"../../openai_app.key\", \"rt\")\n",
    "openai.api_key = f.read()\n",
    "\n",
    "from openai.embeddings_utils import get_embedding, distances_from_embeddings\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n",
    "\n",
    "def get_rows_sorted_by_relevance(question, df):\n",
    "    \"\"\"\n",
    "    Function that takes in a question string and a dataframe containing\n",
    "    rows of text and associated embeddings, and returns that dataframe\n",
    "    sorted from least to most relevant for that question\n",
    "    \"\"\"\n",
    "\n",
    "    # Get embeddings for the question text\n",
    "    question_embeddings = get_embedding(question, engine=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "    # Make a copy of the dataframe and add a \"distances\" column containing\n",
    "    # the cosine distances between each row's embeddings and the\n",
    "    # embeddings of the question\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"embeddings\"] = df_copy[\"embeddings\"].apply(eval).apply(np.array)\n",
    "    df_copy[\"distances\"] = distances_from_embeddings(\n",
    "        question_embeddings,\n",
    "        df_copy[\"embeddings\"].to_list(),\n",
    "        distance_metric=\"cosine\"\n",
    "    )\n",
    "\n",
    "    # Sort the copied dataframe by the distances and return it\n",
    "    # (shorter distance = more relevant so we sort in ascending order)\n",
    "    df_copy.sort_values(\"distances\", ascending=True, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06acf260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token count 57\n",
      "\n",
      "Answer the question based on the context below, and if the \n",
      "question can't be answered based on the context, say \n",
      "\"I don't know\"\n",
      "\n",
      "Context: \n",
      "\n",
      "The USGS Prompt Assessment of Global Earthquakes for Response (PAGER) service estimated a 35 percent probability of economic losses between US$10 billion and US$100 billion. There was a 34 percent probability of economic losses exceeding US$100 billion. The service estimated a 36 percent probability of deaths between 10,000 and 100,000; 26 percent probability of deaths exceeding 100,000. For the second large earthquake, there was a 46 percent probability of deaths between 1,000 and 10,000; 30 percent probability of deaths between 100 and 1,000. The service also estimated a 35 percent percent probability of economic losses between US$1 billion and US$10 billion; 27 percent probability of economic losses between US$10 billion and US$100 billion.Risklayer estimated a death toll of between 23,284 and 105,671. According to geophysics professor, Övgün Ahmet Ercan, \"180,000 people or more may be trapped under the rubble, nearly all of them dead.\" On 11 February, when the death toll was reported at about 28,000, United Nations emergency relief coordinator Martin Griffiths said the death toll was expected to \"more than double\". The World Health Organization said up to 26 million people may have been affected; 15 million in Turkey and 11 million in Syria.Shortly after the earthquakes the Turkish lira value struck a record low of 18.85 against the US dollar, but rebounded to its starting position at the end of the day. Turkish stock markets fell; main equities benchmark fell as much as 5 percent and banks fell 5.5 percent but recovered from the losses. The country's main stock market dropped 1.35 percent on 6 February. The Borsa Istanbul fell 8.6 percent on 7 February, and declined by more than 7 percent on the morning of 8 February before trading was suspended; the exchange then announced it would close for five days. Total costs of the earthquake damage in Turkey was estimated by the TÜRKONFED organization to be $84.1 billion US dollars; $70.75 billion on rebuilding, $10.4 billion loss in national income, and an additional $2.91 billion loss in workforce. The European Bank for Reconstruction and Development said potential losses of up to 1 percent of Turkey's GDP in 2023 could result. About half of residential property in the affected area is thought to be covered by Compulsory Earthquake Insurance. The United Nations Development Programme estimated between 116 million and 210 million tons of debris must be cleared in Turkey.In Syria, the World Bank estimated $5.1 billion worth of damages, excluding economic impact and losses. Nearly half that cost was direct damage to residential buildings and 18 percent on infrastructure. Aleppo Governate, the worst-affected governate, accounted for 45 percent of the damage cost (equivalent to about $2.3 billion), followed by Idlib and Lattakia governates. The amount of destroyed or damaged capital stock is about 10 percent of Syria's GDP.\n",
      "\n",
      "###\n",
      "\n",
      "There was widespread damage in an area of about 350,000 km2 (140,000 sq mi) (about the size of Germany). An estimated 14 million people, or 16 percent of Turkey's population, were affected. Development experts from the United Nations estimated that about 1.5 million people were left homeless.As of 13 March 2023, more than 55,700 deaths were confirmed: more than 48,400 in Turkey, and more than 7,200 in Syria. It is the deadliest earthquake in what is present day Turkey since the 526 Antioch earthquake, making it the deadliest natural disaster in its modern history. It is also the deadliest in what is present day Syria since the 1822 Aleppo earthquake; the deadliest worldwide since the 2010 Haiti earthquake; and the fifth-deadliest of the 21st century. Damages were estimated at over US$100 billion in Turkey and US$5.1 billion in Syria, making them the fourth-costliest earthquakes on record.\n",
      "\n",
      "###\n",
      "\n",
      "Mahase, Elisabeth (7 February 2023). \"Death toll rises after two earthquakes hit Turkey and Syria in 12 hours\" (PDF). BMJ. 380 (380): 304. doi:10.1136/bmj.p304. PMID 36750243. S2CID 256630400.\n",
      "\n",
      "---\n",
      "\n",
      "Question: What were the estimated damages of the 2022 Turkey-Syria earthquake?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Count the number of tokens in the prompt template and question\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the \n",
    "question can't be answered based on the context, say \n",
    "\"I don't know\"\n",
    "\n",
    "Context: \n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "prompt = prompt_template.format(\"contex\", USER_QUESTION)\n",
    "encoded = tokenizer.encode(prompt)\n",
    "token_count = len(tokenizer.encode(prompt_template)) + \\\n",
    "                        len(tokenizer.encode(USER_QUESTION))\n",
    "print(\"token count {}\".format(token_count))\n",
    "\n",
    "\n",
    "# Create a list to store text for context\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"embeddings.csv\", index_col=0)\n",
    "\n",
    "# Create a list to store text for context\n",
    "context_list = []\n",
    "\n",
    "#get_embedding(USER_QUESTION, engine=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# Loop over rows of the sorted dataframe\n",
    "for text in get_rows_sorted_by_relevance(USER_QUESTION, df)[\"text\"].values:\n",
    "    # Append text to context_list if there is enough room\n",
    "    token_count += len(tokenizer.encode(text))\n",
    "    if token_count <= token_limit:\n",
    "        context_list.append(text)\n",
    "    else:\n",
    "        # Break once we're over the token limit\n",
    "        break\n",
    "\n",
    "# Use string formatting to complete the prompt\n",
    "prompt = prompt_template.format(\n",
    "    \"\\n\\n###\\n\\n\".join(context_list),\n",
    "    USER_QUESTION\n",
    ")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83caa2a6",
   "metadata": {},
   "source": [
    "define a create_prompt based on above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6cc1f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def create_prompt(question, df, max_token_count):\n",
    "    \"\"\"\n",
    "    Given a question and a dataframe containing rows of text and their\n",
    "    embeddings, return a text prompt to send to a Completion model\n",
    "    \"\"\"\n",
    "    # Create a tokenizer that is designed to align with our embeddings\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # Count the number of tokens in the prompt template and question\n",
    "    prompt_template = \"\"\"\n",
    "Answer the question based on the context below, and if the question\n",
    "can't be answered based on the context, say \"I don't know\"\n",
    "\n",
    "Context: \n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    current_token_count = len(tokenizer.encode(prompt_template)) + \\\n",
    "                            len(tokenizer.encode(question))\n",
    "\n",
    "    context = []\n",
    "    for text in get_rows_sorted_by_relevance(question, df)[\"text\"].values:\n",
    "\n",
    "        # Increase the counter based on the number of tokens in this row\n",
    "        text_token_count = len(tokenizer.encode(text))\n",
    "        current_token_count += text_token_count\n",
    "\n",
    "        # Add the row of text to the list if we haven't exceeded the max\n",
    "        if current_token_count <= max_token_count:\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097f24d",
   "metadata": {},
   "source": [
    "## TODO 2: Send Custom Text Prompt to Completion Model\n",
    "\n",
    "Using the `prompt` string you created, query an OpenAI `Completion` model to get an answer. Specify a `max_tokens` of 150.\n",
    "\n",
    "If you're getting stuck, you can click to reveal the solution then copy and paste this into the cell below.\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary style=\"cursor: pointer\"><strong>Solution (click to show/hide)</strong></summary>\n",
    "\n",
    "```python\n",
    "COMPLETION_MODEL_NAME = \"text-davinci-003\"\n",
    "response = openai.Completion.create(\n",
    "    model=COMPLETION_MODEL_NAME,\n",
    "    prompt=prompt,\n",
    "    max_tokens=150\n",
    ")\n",
    "answer = response[\"choices\"][0][\"text\"].strip()\n",
    "print(answer)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c912ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"embeddings.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24f3729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETION_MODEL_NAME = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "def answer_question(\n",
    "    question, df, max_prompt_tokens=1800, max_answer_tokens=150\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a question, a dataframe containing rows of text, and a maximum\n",
    "    number of desired tokens in the prompt and response, return the\n",
    "    answer to the question according to an OpenAI Completion model\n",
    "\n",
    "    If the model produces an error, return an empty string\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = create_prompt(question, df, max_prompt_tokens)\n",
    "\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=COMPLETION_MODEL_NAME,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_answer_tokens\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75ca4dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_QUESTION = \"\"\"What were the estimated damages of the 2023 \\\n",
    "Turkey-Syria earthquake?\"\"\";\n",
    "MY_QUESTION = \"\"\"Who owns the twitter?\"\"\";\n",
    "MY_QUESTION = \"\"\"When did Russia invaded Ukraine?\"\"\";\n",
    "\n",
    "answer_question(MY_QUESTION, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05b35a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The estimated damages were over US$100 billion in Turkey and US$5.1 billion in Syria.\n"
     ]
    }
   ],
   "source": [
    "COMPLETION_MODEL_NAME = \"gpt-3.5-turbo-instruct\"\n",
    "response = openai.Completion.create(\n",
    "    model=COMPLETION_MODEL_NAME,\n",
    "    prompt=prompt,\n",
    "    max_tokens=150\n",
    ")\n",
    "answer = response[\"choices\"][0][\"text\"]\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70191209",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations 🎉\n",
    "\n",
    "You have now completed the prompt engineering process using unsupervised ML to get a custom answer from an OpenAI model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
